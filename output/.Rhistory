dic <- word.infor(total_df)
n.all <<- sum(dic)
dic <<- names(dic)
author <<- sort(unique(total_df$clusterid))
K <<- length(author)
L <<- length(dic)
q <- matrix(1:K, ncol = 1)
prior.dic <- apply(q, 1, prior.author.p, df)
prior.author <- table(df$clusterid) / length(df$clusterid)
dim(prior.author)
dim(prior.dic)
output <- rbind(prior.dic, prior.author)
dim(output)
length(author)
length(c("p.title.seen", "p.title.unseen", names(dic), "p.word.unseen", "prior.author"))
names(dic)
dic <- word.infor(total_df)
dim(dic)
row.names(output) <- c("p.title.seen", "p.title.unseen", dic, "p.word.unseen", "prior.author")
p.journal <- function(df, total_df) {
# the total word dictionary
dic.all <- word.infor(total_df)
n.all <<- sum(dic.all)
dic <<- names(dic.all)
author <<- sort(unique(total_df$clusterid))
K <<- length(author)
L <<- length(dic)
q <- matrix(1:K, ncol = 1)
prior.dic <- apply(q, 1, prior.author.p, df)
prior.author <- table(df$clusterid) / length(df$clusterid)
output <- rbind(prior.dic, prior.author)
colnames(output) <- author
row.names(output) <- c("p.title.seen", "p.title.unseen", dic, "p.word.unseen", "prior.author")
return(output)
}
dic.all <- word.infor(total_df)
n.all <<- sum(dic.all)
dic <<- names(dic.all)
dic
author <<- sort(unique(total_df$clusterid))
K <<- length(author)
L <<- length(dic)
q <- matrix(1:K, ncol = 1)
prior.dic <- apply(q, 1, prior.author.p, df)
prior.author <- table(df$clusterid) / length(df$clusterid)
output <- rbind(prior.dic, prior.author)
colnames(output) <- author
row.names(output) <- c("p.title.seen", "p.title.unseen", dic, "p.word.unseen", "prior.author")
ak.train <- p.journal(train, df)
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
for (k in 1:50) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
for (k in 1:40) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
for (k in 1:10) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
k=1
subdic <- word.infor(test[k, ])
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
names(subdic)
names(subdic) %in% dic.all
acc.test <- function(df) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
ak.train <- p.journal(train, df)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
for (k in 1:10) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
for (zzz in 1:length(data.files)) {
df <- trans.data(zzz)
acc <- numeric(10)
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
final[zzz] <- mean(acc)
}
final
)
final <- numeric(length(data.files))
for (zzz in 1:length(data.files)) {
df <- trans.data(zzz)
acc <- numeric(10)
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
final[zzz] <- mean(acc)
}
acc.test <- function(df) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
ak.train <- p.journal(train, df)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
for (k in 1:10) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
df <- trans.data(zzz)
)
zzz
df <- trans.data(zzz)
acc <- numeric(10)
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
acc.test <- function(df) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
ak.train <- p.journal(train, df)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
for (k in 1:10) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
}
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
ak.train <- p.journal(train, df)
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
for (k in 1:10) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
for (k in 1:10) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
for (k in 1:nrow(test)) {
#  for (k in 1:10) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
print(head(post.p[k,],2))
}
acc.test <- function(df) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
ak.train <- p.journal(train, df)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
acc[zz] <- acc.test(df)
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
final[zzz] <- mean(acc)
final <- numeric(length(data.files))
for (zzz in 1:length(data.files)) {
df <- trans.data(zzz)
acc <- numeric(10)
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
final[zzz] <- mean(acc)
}
final
final <- numeric(length(data.files))
for (zzz in 1:length(data.files)) {
df <- trans.data(zzz)
acc <- numeric(10)
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
final[zzz] <- mean(acc)
}
final
mean(final)
zzz=2
final <- numeric(length(data.files))
for (zzz in 1:length(data.files)) {
df <- trans.data(zzz)
acc <- numeric(10)
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
final[zzz] <- mean(acc)
}
final
mean(final)
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/output")
attach("CleanData.RData")
data.lib <- "~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/data/nameset"
data.files <- list.files(path = data.lib, "*.txt")
final <- matrix(NA, nrow = 10, ncol = length(data.files))
for (zzz in 1:length(data.files)) {
df <- trans.data(zzz)
acc <- numeric(10)
for(zz in 1:10) {
acc[zz] <- acc.test(df)
}
final[zzz] <- mean(acc)
}
apply(final, 2, mean)
final
final <- matrix(NA, nrow = 10, ncol = length(data.files))
for (j in 1:length(data.files)) {
df <- trans.data(j)
for(i in 1:10) {
final[i, j] <- acc.test(df)
}
}
apply(final, 2, mean)
final
mean(apply(final, 2, mean))
?write.csv
write.csv(output, file = "Journal.accuracy.result.csv")
output <- rbind(final, acc.mean)
acc.mean <- apply(final, 2, mean)
acc.mean
mean(apply(final, 2, mean))
output <- rbind(final, acc.mean)
write.csv(output, file = "Journal.accuracy.result.csv")
j=5
df <- trans.data(j)
for(i in 1:10) {
final[i, j] <- acc.test(df)
}
final[,5]
ifelse(1>2,1,2)
word.infor <- function(df, j_p) {
dic <- ifelse(j_p == "j", Corpus(VectorSource(df$Journal)), Corpus(VectorSource(df$Paper)))
dic <- tm_map(dic, content_transformer(tolower))
dic <- tm_map(dic, removePunctuation)
dic <- tm_map(dic, removeWords, stopwords("english"))
dic <- lapply(dic, dec)
dic <- unlist(dic)
dic <- sort(table(dic), decreasing = T)
if ("" %in% names(dic)) { dic <- dic[-which(names(dic) == "")]}
return(dic)
}
read.data <- function(input) {
df <- paste(data.lib, input, sep="/")
df <- data.frame(scan(df, what = list(Coauthor = "", Paper = "", Journal = ""),
sep=">", quiet=TRUE), stringsAsFactors=F)
df$clusterid <- sub("_.*","",df$Coauthor) # author_id
df$citationid <- sub(".*_(\\w*)\\s.*", "\\1", df$Coauthor) # paper_id
df$coauthor <- gsub("<","",sub("^.*?\\s","", df$Coauthor))
df$Paper <- gsub("<","",df$Paper)
return(df)
}
trans.data <- function(j) {
ori.data <- data_list[[j]]
output <- data.frame(matrix(NA, nrow = length(ori.data), ncol = 3))
colnames(output) <- c("Paper", "Journal", "clusterid")
for (i in 1:length(ori.data)) {
output$Paper[i] <- ori.data[[i]][[4]]
output$Journal[i] <- ori.data[[i]][[5]]
output$clusterid[i] <- ori.data[[i]][[1]]
}
return(output)
}
dec <- function(z) {
return(strsplit(z, split = " "))
}
word.infor <- function(df, j_p) {
dic <- ifelse(j_p == "j", Corpus(VectorSource(df$Journal)), Corpus(VectorSource(df$Paper)))
dic <- tm_map(dic, content_transformer(tolower))
dic <- tm_map(dic, removePunctuation)
dic <- tm_map(dic, removeWords, stopwords("english"))
dic <- lapply(dic, dec)
dic <- unlist(dic)
dic <- sort(table(dic), decreasing = T)
if ("" %in% names(dic)) { dic <- dic[-which(names(dic) == "")]}
return(dic)
}
find_loc <- function(input) {
output <- rep(0, L)
for (i in 1:length(input)) {
index <- which(dic == names(input)[i])
output[index] <- input[i] / sum(input)
}
return(output)
}
prior.author.p <- function(j, df, j_p) {
subdata <- subset(df, clusterid == author[j])
subdic <- word.infor(subdata, j_p)
# probability of the author publish a paper on a journal with a seen word in the journal title
p.title.seen <- sum(subdic[subdic >= 2]) / sum(subdic)
# probability of the author publish a paper on a journal with a unseen word in the journal title
p.title.unseen <- 1 - p.title.seen
# probability of the author publish a paper on a journal with kth word condition on the jornal name has a seen word
p.word.seen <- find_loc(subdic)
# probability of the author publish a paper on a journal with kth word condition on the jornal name has a unseen word
p.word.unseen <- 1 / (n.all)
output <- c(p.title.seen, p.title.unseen, p.word.seen, p.word.unseen)
return(output)
}
split_data <- function(df){
author <- unique(df$clusterid)
output <- numeric(0)
for(i in 1:length(author)) {
obs <- which(df$clusterid == author[i])
random <- sample(obs, round(length(obs)/2))
output <- c(output, random)
}
return(output)
}
p.journal <- function(df, total_df, j_p) {
# the total word dictionary
dic.all <- word.infor(total_df, j_p)
n.all <<- sum(dic.all)
dic <<- names(dic.all)
author <<- sort(unique(total_df$clusterid))
K <<- length(author)
L <<- length(dic)
q <- matrix(1:K, ncol = 1)
prior.dic <- apply(q, 1, prior.author.p, df, j_p)
prior.author <- table(df$clusterid) / length(df$clusterid)
output <- rbind(prior.dic, prior.author)
colnames(output) <- author
row.names(output) <- c("p.title.seen", "p.title.unseen", dic, "p.word.unseen", "prior.author")
return(output)
}
j=1
df <- trans.data(j)
acc.test(df)
acc.test <- function(df, j_p) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
ak.train <- p.journal(train, df, j_p)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ], j_p)
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
acc.test(df, "j")
acc.test <- function(df, j_p) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
ak.train <- p.journal(train, df, j_p)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ], j_p)
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
acc.test(df, "j")
word.infor <- function(df, j_p) {
dic <- ifelse(j_p == "j", Corpus(VectorSource(df$Journal)), Corpus(VectorSource(df$Paper)))
dic <- tm_map(dic, content_transformer(tolower))
dic <- tm_map(dic, removePunctuation)
dic <- tm_map(dic, removeWords, stopwords("english"))
dic <- lapply(dic, dec)
dic <- unlist(dic)
dic <- sort(table(dic), decreasing = T)
if ("" %in% names(dic)) { dic <- dic[-which(names(dic) == "")]}
return(dic)
}
acc.test <- function(df, j_p) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
ak.train <- p.journal(train, df, j_p)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ], j_p)
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
acc.test(df, "j")
library("NLP")
library("tm")
source(p.function.R)
source(p.function.R)
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/output")
attach("CleanData.RData")
source(p.function.R)
source("p.function.R")
source("p.function.R", local=T)
