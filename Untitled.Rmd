---
title: "Naive Bayes Main Script"
author: "Nanjun Wang"
output:
  html_document:
    theme: readable
    toc: TRUE
    smart: TRUE
    toc_depth: 3
    toc_float: FALSE
    fig_width : 7
    fig_height: 5
    fig_retina : 2
---

# Paper 2, coauthor part:

## Step 0: Load the packages, specify directories
```{r}
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14")
library(NLP)
library(tm)
library(knitr)
```

## Step 1: Load and process the data
In each record of the nameset, we need to extract the information of canonical author id and coauthors. We use cleaned data posted by the class and M matrix in paper 6 constructed by my teammate to accomplish that. 

```{r}
## Load data_list
load("../output/CleanData.RData")

## Load M matrix
load("../output/M.RData")

## Extract canonical author id
label <- function(data_list, num){
  nameset <- data_list[[num]]
  n <- length(nameset)
  label <- NA
  for(i in 1:n){
    label[i] <- nameset[[i]][1]
  }
  return(unlist(label))
}

labels <- NULL
for(i in 1:14){
  l <- label(data_list = data_list, num = i)
  labels[[i]] <- l
}

## Extract coauthors informartion and
## combine it with canonical author id
coauthors <- NULL
for(i in 1:14){
  obs <- length(data_list[[i]])
  d <- ncol(M.Matrix[[i]])
  labels[[i]] <- as.matrix(labels[[i]])
  coauthors[[i]] <- cbind(labels[[i]],M.Matrix[[i]][1:obs, (obs+1) : d])
  coauthors[[i]] <- cbind(coauthors[[i]], rep(1,obs))
}

```

## Step 2: Naive Bayes model
Following the section 2 in the paper, we use each author's past citations as the training data to estimate the model parameters. Based on the parameters estimates, we use the Bayes rule to calculate the probability that each name entry X$_i$ would have generated the input citation.

### 2.1 Training process
Given an input test citaiton C with the ommissuib of the query author, the traget function is to find a name entry X$_i$ in the citation database with the maximal posterior probability of producing the citation C, i.e., $max_iP(X_i|C) (1)$. After the decomposition of (1), it turns out in the training process we need to extimates seven parameters: $P(X_i)$, $P(N|X_i)$, $P(Co|X_i)$, $P(Seen|Co,X_i)$, $P(Unseen|Co,X_i)$, $P(A_1k|Seen, Co, X_i)$, $P(A_1k|Unseen, Co, X_i)$.
We have already split all records in the nameset for the experiments in section 4. Here, we only use A Gupta's training data as an example to show how training process goes.
```{r}
source("../lib/wang_nb_train.R")
train_1 <- read.csv("../output/Exp1/train_1.csv")
train_1 <- as.matrix(train_1)
model <- nb_train(train_x = train_1[,-1], train_y = train_1[,1])
```

### 2.2 Testing process
We still use A Gupta's test data as an example to show how test process goes
```{r}
source("../lib/wang_nb_test.R")
test_1 <- read.csv("../output/Exp1/train_1.csv")
prediction <- nb_test(model = model, test_data = test_1[,-1])
mean(test_1[,1] == prediction)
```

## Step 3: Datasets and experiment design
Following the section 4, we split all records in the nameset into training data and test data as the way suggested in the paper. We conduct ten exmperiments on each record and calulate the mean and the standard deviation of accuracy.
```{r, echo=FALSE}
source("../lib/wang_read_files.R")
source("../lib/wang_experiments.R")

accuracy_table <- matrix(NA,2,14)
for(i in 1:14){
  nameset <- read_files(i)
  result <- experiment(nameset$train_files, nameset$test_files)
  accuracy_table[1,i] <- result$mean
  accuracy_table[2,i] <- result$StdDev
}
accuracy_table <- as.data.frame(accuracy_table)
Name <- c("AGupta", "Akumar", "CCchen", "DJohnson",  "JLee", "JMartin", "JRobinson", "JSmith", "KTanaka", "MBrown", "MJones", "MMiller", "SLee", "Ychen")
colnames(accuracy_table) <- Name
rownames(accuracy_table) <- c("Mean", "StdDev")
kable( accuracy_table, digits = 2)
```

# Paper 2, journal title and paper title part:

## Step 1: Load and process the data

```{r, warning = FALSE}
source('../lib/p.function.R', local = T)
AGupta <- trans.data(1)
```


## Step 2: Feature design

Following the section 2.2 in the paper, we want to use paper titles and journal titles to design features for citations. As the notation used in the paper, we want to find a $m$-dimensional citation vector $\alpha_i$ for each citation $i$, $i=1,...,n$. In this dataset, $n=$ `r nrow(AGupta)`. We study the Naive Bates method as suggested in the paper.

The conditional probabilities we used in this method are as follows:

\begin{aligned}
\mbox{P}(A_{2k}|Seen, X_{i}) &= \frac{\mbox{Number of times that word $k$ appears in the title that belongs to $X_{i}$ }}{\mbox{Total number of words in the title that belongs to $X_{i}$}}\\
\\
\mbox{P}(Seen|X_{i}) &= \frac{\mbox{Total number of words that appears more than twice in the title that belongs to $X_{i}$ }}{\mbox{Total number of words in the title that belongs to $X_{i}$}}\\
\\
\mbox{P}(A_{2k}|Unseen, X_{i}) &= \frac{1}{\mbox{Total number of words in the title} - \mbox{Total number of words in the title that belongs to $X_{i}$}}\\
\\
\mbox{P}(Unseen|X_{i}) &= \frac{\mbox{Total number of words that appears only once in the title that belongs to $X_{i}$}}{\mbox{Total number of words in the title that belongs to $X_{i}$}}\\
&= 1-\mbox{P}(A_{2k}|Seen, X_{i})\\
\\
\mbox{P}(A_{2}|X_{i} &= \mbox{P}(A_{21}|X_{i})\dots\mbox{P}(A_{2k}|X_{i})\dots\mbox{P}(A_{2K}|X_{i})\\
\mbox{where}\\
\mbox{P}(A_{2k}|X_{i}) &= \mbox{P}(A_{2k}, Seen|X_{i}) + \mbox{P}(A_{2k}, Unseen|X_{i})\\
&= \mbox{P}(A_{2k}|Seen, X_{i}) \times \mbox{P}(Seen|X_{i}) + \mbox{P}(A_{2k}|Unseen, X_{i}) \times \mbox{P}(Unseen|X_{i})
\end{aligned}

We first create a vocabulary. Here we collect unique words from all titles in the document， then we remove pre-defined stopwords, the words like “a”, “the”, “in”, “I”, “you”, “on”, etc, which do not provide much useful information. Thus, we get the vocabulary list with frequency.

```{r}
# the vocabulary list for journal title with frequency
Dic.j <- word.infor(AGupta, "j")
# the vocabulary list for paper title with frequency
Dic.p <- word.infor(AGupta, "p")
```

Saperate data for training and testing.

```{r}
index <- read.csv("../output/random_agupta.csv")
index <- index$x
train <- AGupta[index, ]
test <- AGupta[-index, ]
```

## Step 3: Clustering and accuracy:

Following suggestion in the paper, we use conditional probability to calculate posterior parobability for journal title for each observation, and choose the cluster that maximaize the posterior parobability.

```{r}
source('../lib/title.test.R', local = T)
result.class.j <- test.result(index, AGupta, "j")
(acc.j <- mean(as.numeric(result.class.j) == test$clusterid))
time.journal <- system.time(test.result(index, AGupta, "j"))
```

We can also calculate posterior parobability for paper title for each observation, and choose the cluster that maximaize the posterior parobability.

```{r}
result.class.p <- test.result(index, AGupta, "p")
(acc.p <- mean(as.numeric(result.class.p) == test$clusterid))
time.paper <- system.time(test.result(index, AGupta, "p"))
```

## Datasets and experiment design

Following the section 4, we split all records in the nameset into training data and test data as the way suggested in the paper. We conduct ten exmperiments on each record and calulate the mean and the standard deviation of accuracy.

```{r}
# journal title part
final.j <- matrix(NA, nrow = 10, ncol = 14)
for (j in 1:14) {
  df <- trans.data(j)
  for(i in 1:10) {
    final.j[i, j] <- acc.test(df, "j")
  }
}
acc.mean.j <- apply(final.j, 2, mean)
acc.sd.j <- apply(final.j, 2, sd)
accuracy_j_table <- as.data.frame(rbind(acc.mean.j, acc.sd.j))
colnames(accuracy_j_table) <- Name
row.names(accuracy_j_table) <- c("Mean", "StdDev")
kable(accuracy_j_table, digits = 2)

# paper title part
final.p <- matrix(NA, nrow = 10, ncol = 14)
for (j in 1:14) {
  df <- trans.data(j)
  for(i in 1:10) {
    final.p[i, j] <- acc.test(df, "p")
  }
}
acc.mean.p <- apply(final.p, 2, mean)
acc.sd.p <- apply(final.p, 2, sd)
accuracy_p_table <- as.data.frame(rbind(acc.mean.p, acc.sd.j))
colnames(accuracy_p_table) <- Name
row.names(accuracy_p_table) <- c("Mean", "StdDev")
kable(accuracy_p_table, digits = 2)
```

## Step 5: Evaluation

To evaluate the performance of the method, it is required to calculate the degree of agreement between a set of system-output partitions and a set of true partitions. In general, the agreement between two partitioins is measured for a pair of entities within partitions. The basic unit for which pair-wise agreement is assessed is a pair of entities (authors in our case) which belongs to one of the four cells in the following table (Kang et at.(2009)):

\includegraphics[width=500pt]{matching_matrix.png}

Let $M$ be the set of machine-generated clusters, and $G$ the set of gold standard clusters. Then. in the table, for example, $a$ is the number of pairs of entities that are assigned to the same cluster in each of $M$ and $G$. Hence, $a$ and $d$ are interpreted as agreements, and $b$ and $c$ disagreements. When the table is considered as a confusion matrix for a two-class prediction problem, the standard "Precision", "Recall","F1", and "Accuracy" are defined as follows.

$$
\begin{aligned}
\mbox{Precision} &=\frac{a}{a+b}\\
\mbox{Recall}&=\frac{a}{a+c}\\
\mbox{F1} &=\frac{2\times\mbox{Precision}\times\mbox{Recall}}{\mbox{Precision}+\mbox{Recall}}\\
\mbox{Accuracy}&=\frac{a+d}{a+b+c+d}
\end{aligned}
$$

```{r, warning = FALSE}
source('../lib/evaluation_measures.R', local = T)
source('../lib/test_result.R', local = T)
result.class.c <- prediction
matching_matrix_coauthor <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.c)
performance_coauthor <- performance_statistics(matching_matrix_coauthor)
matching_matrix_journal <- matching_matrix(as.numeric(df$clusterid[-index]),result.class.j)
performance_journal <- performance_statistics(matching_matrix_journal)
matching_matrix_paper <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.p)
performance_paper <- performance_statistics(matching_matrix_paper)
time.coauthor <- 3.1
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
                         precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
                         recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
                         f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
                         accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accuracy),
                         time = c(time.coauthor, time.paper[3], time.journal[3]))
kable(compare_df,caption = "Comparision of performance for two clustering methods", digits = 2)
```
