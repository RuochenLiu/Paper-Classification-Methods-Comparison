c = tune_par_cnn150$best_cost))
system.time(test.fn(data_test = test_cnn150[,-c(1,152)],
bestfit_cnn150))
load("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/spr2017-proj3-group3/output/err_cv.RData")
train.fn <- function(dat_train, label_train, c=1000){
library(e1071)
fit_svm <- svm(dat_train, as.factor(label_train), data = cbind(dat_train, label_train),
cost = c,
kernel = "linear",
scale = F,
probability = T)
return(fit = fit_svm)
}
#ff <- train.fn(dat_train = new_data_train[,-c(1,5002)], label_train = new_data_train[,1])
train.fn <- function(dat_train, label_train, c=1000){
library(e1071)
fit_svm <- svm(dat_train, as.factor(label_train), data = cbind(dat_train, label_train),
cost = c,
kernel = "linear",
scale = F,
probability = T)
return(fit = fit_svm)
}
#ff <- train.fn(dat_train = new_data_train[,-c(1,5002)], label_train = new_data_train[,1])
cv.function <- function(X.train, y.train, c, K){
library(e1071)
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.error <- rep(NA, K)
error <- data.frame(NA)
cost <- c
for(i in 1:length(c)){
for (j in 1:K){
train.data <- X.train[s != j,]
train.label <- y.train[s != j]
test.data <- X.train[s == j,]
test.label <- y.train[s == j]
fit <- svm(X.train, as.factor(y.train), data = cbind(X.train, y.train),
cost = c[i],
kernel = "linear",
scale = F)
pred <- predict(fit, test.data)
cv.error[j] <- mean(pred != test.label)
}
error[i,1] <- c[i]
error[i,2] <- mean(cv.error)
error[i,3] <- sd(cv.error)
colnames(error) <- c("cost","mean","sd")
}
return(list( error, best_cost = error[which.min(error[,2]),1]))
}
#cv.result <- cv.function(train[,-1], train[,1] ,c = c(10^3,10^4,10^5), K = 5)
#cv.result
#train.tm <- system.time(cv.function(train[,-1], train[,1] ,c = c(10, 10^2,10^3), K = 5))
svm_adv <- function(file_dir){
###load libraries
library(dplyr)
###read feature files
prediction_inception <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/spr2017-proj3-group3/output/prediction_inceptionV3.csv")
label <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/labels.csv")
label$V1 <- as.numeric(label$V1)
dataset <- read.csv(file_dir)
dataset <- data.frame(t(dataset))
dataset <- cbind(label,dataset)
file_rowname <- rownames(dataset)[1]
value <- "jpg"
if(grepl(value, file_rowname)){
dataset$image <- rownames(dataset)
} else{
dataset$image <- paste(rownames(dataset),".jpg", sep = "")
}
###set test and train data
test_data <- dataset %>% filter(dataset$image %in% prediction_inception$image)
train_data <- dataset %>% filter(!dataset$image %in% prediction_inception$image)
###train model
#Cross validaiton---tunning parameters
tune_par_cnn150 <- cv.function(X.train = train_data[,-c(1,152)],
y.train = train_data[,1],
c = c(0.001,0.01,0.1),
K = 5)
#prediction error after tunning parameters
bestfit_cnn150 <- train.fn(dat_train = train_data[,-c(1,152)],
label_train = train_data[,1],
c = tune_par_cnn150$best_cost)
return(model = bestfit_cnn150)
}
svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_150.csv")
bestfit_cnn150 <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_150.csv")
ccc150 <- read_set_fn(file_name = "cnn_features_150.csv")
read_set_fn <- function(file_name){
library(dplyr)
###read feature files
file_dir<- paste("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/",file_name, sep = "")
prediction_inception <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/spr2017-proj3-group3/output/prediction_inceptionV3.csv")
label <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/labels.csv")
label$V1 <- as.numeric(label$V1)
dataset <- read.csv(file_dir)
dataset <- data.frame(t(dataset))
dataset <- cbind(label,dataset)
file_rowname <- rownames(dataset)[1]
value <- "jpg"
if(grepl(value, file_rowname)){
dataset$image <- rownames(dataset)
} else{
dataset$image <- paste(rownames(dataset),".jpg", sep = "")
}
###set test and train data
test_data <- dataset %>% filter(dataset$image %in% prediction_inception$image)
train_data <- dataset %>% filter(!dataset$image %in% prediction_inception$image)
return(list(dataset,test_data, train_data))
}
ccc150 <- read_set_fn(file_name = "cnn_features_150.csv")
test_cnn150 <- ccc150[[2]]
train_cnn150 <- ccc150[[3]]
prediction <- predict(test_cnn150[,-c(1.152)],bestfit_cnn150)
prediction <- predict(test_cnn150[,-c(1,152)],bestfit_cnn150)
pred_best_cnn150 <- test.fn(data_test = test_cnn150[,-c(1,152)],
bestfit_cnn150)
test.fn <- function(data_test, fit_model){
library(e1071)
pred <- predict(fit_model, data_test, probability = T)
pred.prob <- attr(pred, 'probabilities')
return(pred.prob)
}
pred_best_cnn150 <- test.fn(data_test = test_cnn150[,-c(1,152)],
bestfit_cnn150)
head(pred_best_cnn150)
setwd("~/Desktop/Columbia/Spring 2017/ADS/Project 3")
pred_best_cnn150 <- cbind(test_cnn150$image, pred_best_cnn150)
View(pred_best_cnn150)
colnames(pred_best_cnn150) <- c("image", "friedChicken","labradoodle")
View(pred_best_cnn150)
write.csv(pred_best_cnn150, file = "prediction_svm_cnn150.csv")
svm_adv <- function(file_dir,k,run.cv){
# file_dir: path to features.csv
# k: number of features
###load libraries
library(dplyr)
library(e1071)
#library(data.table)
###read feature files
#prediction_inception <- read.csv("/output/prediction_inceptionV3.csv")
test_ids <- fread("output/prediction_inceptionV3.csv")
label <- fread("data/labels.csv")
label$V1 <- as.numeric(label$V1)
dataset <- fread(file_dir)
dataset <- data.frame(t(dataset))
#dataset <- cbind(label,dataset)
dataset$label <- label$V1
file_rowname <- rownames(dataset)
value <- "jpg"
if(grepl(value, file_rowname[1])){
dataset$image <- rownames(dataset)
} else{
dataset$image <- paste(rownames(dataset),".jpg", sep = "")
}
###set test and train data
#test_data <- dataset %>% filter(dataset$image %in% test_ids$image)
train_data <- dataset %>% filter(!dataset$image %in% test_ids$image)
###train model
#Cross validaiton---tunning parameters
if(run.cv){
tune_par_cnn150 <- cv.function(X.train = train_data[,1:k],
y.train = train_data[,'label'],
c = c(0.0001,0.0003,0.001),
K = 5)
best.cost <- tune_par_cnn150$best_cost
} else{
best.cost <- 0.001
}
#prediction error after tunning parameters
bestfit_cnn150 <- train.fn(dat_train = train_data[,1:k],
label_train = train_data[,'label'],
c = best.cost)
return(bestfit_cnn150)
}
bestfit <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 352, run.cv = T)
library(data.table)
bestfit <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 352, run.cv = T)
bestfit <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 352, run.cv = T)
bestfit <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 352, run.cv = T)
cv.function <- function(X.train, y.train, c, K){
library(e1071)
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.error <- rep(NA, K)
error <- data.frame(NA)
cost <- c
for(i in 1:length(c)){
for (j in 1:K){
train.data <- X.train[s != j,]
train.label <- y.train[s != j]
test.data <- X.train[s == j,]
test.label <- y.train[s == j]
fit <- svm(X.train, as.factor(y.train), data = cbind(X.train, y.train),
cost = c[i],
kernel = "linear",
scale = F)
pred <- predict(fit, test.data)
cv.error[j] <- mean(pred != test.label)
}
error[i,1] <- c[i]
error[i,2] <- mean(cv.error)
error[i,3] <- sd(cv.error)
colnames(error) <- c("cost","mean","sd")
}
return(list( error, best_cost = error[which.min(error[,2]),1]))
}
#cv.result <- cv.function(train[,-1], train[,1] ,c = c(10^3,10^4,10^5), K = 5)
#cv.result
#train.tm <- system.time(cv.function(train[,-1], train[,1] ,c = c(10, 10^2,10^3), K = 5))
bestfit <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 352, run.cv = T)
library(data.table)
bestfit <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 350, run.cv = T)
bestfit <- svm_adv(file_dir = "../cnn_features_350.csv", k = 350, run.cv = T)
bestfit <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 350, run.cv = T)
train.fn <- function(dat_train, label_train, c=1000){
library(e1071)
fit_svm <- svm(dat_train, as.factor(label_train), data = cbind(dat_train, label_train),
cost = c,
kernel = "linear",
scale = F,
probability = T)
return(fit = fit_svm)
}
#ff <- train.fn(dat_train = new_data_train[,-c(1,5002)], label_train = new_data_train[,1])
bestfit <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 350, run.cv = T)
read_set_fn <- function(file_name){
library(dplyr)
###read feature files
file_dir<- paste("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/",file_name, sep = "")
prediction_inception <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/spr2017-proj3-group3/output/prediction_inceptionV3.csv")
label <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/labels.csv")
label$V1 <- as.numeric(label$V1)
dataset <- read.csv(file_dir)
dataset <- data.frame(t(dataset))
dataset <- cbind(label,dataset)
file_rowname <- rownames(dataset)[1]
value <- "jpg"
if(grepl(value, file_rowname)){
dataset$image <- rownames(dataset)
} else{
dataset$image <- paste(rownames(dataset),".jpg", sep = "")
}
###set test and train data
test_data <- dataset %>% filter(dataset$image %in% prediction_inception$image)
train_data <- dataset %>% filter(!dataset$image %in% prediction_inception$image)
return(list(dataset,test_data, train_data))
}
read_set_fn(file_name = "cnn_fearures_350")
dataset <- read_set_fn(file_name = "cnn_features_350")
dataset <- read_set_fn(file_name = "cnn_features_350.csv")
ttt <- dataset[[2]]
test_result <- predict(bestfit, ttt[,-c(1,352)])
test_result <- test.fn(ttt[,-c(1,352)],
bestfit)
test.fn <- function(data_test, fit_model){
library(e1071)
pred <- predict(fit_model, data_test, probability = T)
pred.prob <- attr(pred, 'probabilities')
return(pred.prob)
}
test_result <- test.fn(ttt[,-c(1,352)],
bestfit)
write.csv(test_result, "prediction_svm_cnn350.csv")
test_result <- cbind(ttt$image, test_result)
colnames(test_result) <- c("image", "friedChicken","labradoodle")
write.csv(test_result, "prediction_svm_cnn350.csv")
test_result
ttt
test_result$friedChicken
test_result[,2]
pred <- as.numeric(test_result[,2])
pred
(pred<=0.5)==ttt[[1]]
sum((pred<=0.5)==ttt[[1]])
sum((pred<=0.5)==ttt[[1]])/400
index <- (pred<=0.5)==ttt[[1]]
which(index != T)
system.time(svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/cnn_features_350.csv", k = 350, run.cv = T))
system.time(test.fn(ttt[,-c(1,352)],
bestfit))
bestfit_sift1100 <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/sift_features_1100.csv", k = 1100, run.cv = T)
data_sift1100 <- read_set_fn(file_name = "sift_features_1100.csv")
test_sift1100 <- data_sift1100[[2]]
pred.best_sift1100 <- test.fn(test_sift1100[,-c(1,1102)],
bestfit_sift1000)
pred.best_sift1100 <- test.fn(test_sift1100[,-c(1,1102)],
bestfit_sift1100)
pred_svm_sift1000 <- cbind(test_sift1100$image, pred_svm_sift1100)
pred_svm_sift1100 <- cbind(test_sift1100$image, pred_svm_sift1100)
pred.best_sift1100 <- test.fn(test_sift1100[,-c(1,1102)],
bestfit_sift1100)
pred_svm_sift1100 <- cbind(test_sift1100$image, pred_svm_sift1100)
pred_best_sift1100 <- cbind(test_sift1100$image, pred_best_sift1100)
pred_best_sift1100 <- test.fn(test_sift1100[,-c(1,1102)],
bestfit_sift1100)
pred_best_sift1100 <- cbind(test_sift1100$image, pred_best_sift1100)
colnames(pred_best_sift1100) <- c("image", "friedChicken","labradoodle")
write.csv(pred_best_sift1100, file = "prediction_svm_sift1100")
write.csv(pred_best_sift1100, file = "prediction_svm_sift1100.csv")
system.time(svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/sift_features_1100.csv", k = 1100, run.cv = T))
system.time(test.fn(test_sift1100[,-c(1,1102)],
bestfit_sift1100))
mean((pred_best_sift1100[,2] <= 0.5) == test_sift1100[[1]])
system.time(best_sift5000 <- svm_adv(file_dir = "/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/training_data/sift_features/sift_features.csv", k = 5000, run.cv = T))
system.time( prob <- test.fn(data_test = test_svm_sift[,-c(1,5002)],
fit_model = fit))
sss <- read_set_fn(file_name = 'sift_features.csv')
test_svm_sift <- sss[[2]]
train_svm_sift <- sss[[3]]
system.time( prob <- test.fn(data_test = test_svm_sift[,-c(1,5002)],
fit_model = bestfit_sift5000))
system.time( prob <- test.fn(data_test = test_svm_sift[,-c(1,5002)],
fit_model = best_sift5000))
sss1000 <- read_set_fn(file_name = "sift_features_1000.csv")
test_sift1000 <- sss1000[[2]]
train_sift1000 <- sss1000[[3]]
system.time(fit_svm_sift1000 <- train.fn(dat_train = train_sift1000[,-c(1,1002)],
label_train = train_sift_1000[,1], c = 1000))
system.time(fit_svm_sift1000 <- train.fn(dat_train = train_sift1000[,-c(1,1002)],
label_train = train_sift1000[,1], c = 1000))
ccc <- read_set_fn(file_name = 'cnn_features.csv')
test_cnn <- ccc[[2]]
train_cnn <- ccc[[3]]
system.time(fit_svm_cnn <- train.fn(dat_train = train_cnn[,-c(1,2050)],
label_train = train_cnn[,1], c = 1000))
cv.function <- function(X.train, y.train, c, K){
library(e1071)
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.error <- rep(NA, K)
error <- data.frame(NA)
cost <- c
for(i in 1:length(c)){
for (j in 1:K){
train.data <- X.train[s != j,]
train.label <- y.train[s != j]
test.data <- X.train[s == j,]
test.label <- y.train[s == j]
fit <- svm(X.train, as.factor(y.train), data = cbind(X.train, y.train),
cost = c[i],
kernel = "linear",
scale = F)
pred <- predict(fit, test.data)
cv.error[j] <- mean(pred != test.label)
}
error[i,1] <- c[i]
error[i,2] <- mean(cv.error)
error[i,3] <- sd(cv.error)
colnames(error) <- c("cost","mean","sd")
}
return(list( error, best_cost = error[which.min(error[,2]),1]))
}
#cv.result <- cv.function(train[,-1], train[,1] ,c = c(10^3,10^4,10^5), K = 5)
#cv.result
#train.tm <- system.time(cv.function(train[,-1], train[,1] ,c = c(10, 10^2,10^3), K = 5))
ccc150 <- read_set_fn(file_name = "cnn_features_150.csv")
read_set_fn <- function(file_name){
library(dplyr)
###read feature files
file_dir<- paste("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/",file_name, sep = "")
prediction_inception <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/spr2017-proj3-group3/output/prediction_inceptionV3.csv")
label <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/labels.csv")
label$V1 <- as.numeric(label$V1)
dataset <- read.csv(file_dir)
dataset <- data.frame(t(dataset))
dataset <- cbind(label,dataset)
file_rowname <- rownames(dataset)[1]
value <- "jpg"
if(grepl(value, file_rowname)){
dataset$image <- rownames(dataset)
} else{
dataset$image <- paste(rownames(dataset),".jpg", sep = "")
}
###set test and train data
test_data <- dataset %>% filter(dataset$image %in% prediction_inception$image)
train_data <- dataset %>% filter(!dataset$image %in% prediction_inception$image)
return(list(dataset,test_data, train_data))
}
ccc150 <- read_set_fn(file_name = "cnn_features_150.csv")
ccc150 <- read_set_fn(file_name = "cnn_features_150.csv")
test_cnn150 <- ccc150[[2]]
train_cnn150 <- ccc150[[3]]
cv.function(X.train = train_cnn150[,-c(1,152)], y.train = train_cnn150[,1],)
cv.function(X.train = train_cnn150[,-c(1,152)], y.train = train_cnn150[,1], c = 10^c(-1:2), K = 5)
cv.function(X.train = train_cnn150[,-c(1,152)], y.train = train_cnn150[,1], c = 10^c(5,6,7), K = 5)
cv.function(X.train = train_cnn150[,-c(1,152)], y.train = train_cnn150[,1], c = 10^c(8,9,10), K = 5)
tune.svm(x = train_cnn150[,-c(1,152)], y = train_cnn150[,1], data = train_cnn150, cost  = 10^c(-1:2))
tttune <- tune.svm(x = train_cnn150[,-c(1,152)], y = train_cnn150[,1], data = train_cnn150, cost  = 10^c(-1:2))
tttune$performances
ccc150 <- read_set_fn(file_name = "cnn_features_150.csv")
read_set_fn <- function(file_name){
library(dplyr)
###read feature files
file_dir<- paste("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/",file_name, sep = "")
prediction_inception <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/spr2017-proj3-group3/output/prediction_inceptionV3.csv")
label <- read.csv("/Users/ouminamikun/Desktop/Columbia/Spring 2017/ADS/Project 3/labels.csv")
label$V1 <- as.numeric(label$V1)
dataset <- read.csv(file_dir)
dataset <- data.frame(t(dataset))
dataset <- cbind(label,dataset)
file_rowname <- rownames(dataset)[1]
value <- "jpg"
if(grepl(value, file_rowname)){
dataset$image <- rownames(dataset)
} else{
dataset$image <- paste(rownames(dataset),".jpg", sep = "")
}
###set test and train data
test_data <- dataset %>% filter(dataset$image %in% prediction_inception$image)
train_data <- dataset %>% filter(!dataset$image %in% prediction_inception$image)
return(list(dataset,test_data, train_data))
}
ccc150 <- read_set_fn(file_name = "cnn_features_150.csv")
ccc150 <- read_set_fn(file_name = "cnn_features_150.csv")
test_cnn150 <- ccc150[[2]]
train_cnn150 <- ccc150[[3]]
train.fn <- function(dat_train, label_train, c=1000){
### Train a support vector machine (svm) using processed features from training images
### Input:
###  -  processed features from images
###  -  class labels for training images
### Output: training model specification
### load libraries
library(e1071)
### Train with svm model
fit_svm <- svm(dat_train, as.factor(label_train), data = cbind(dat_train, label_train),
cost = c,
kernel = "linear",
scale = F,
probability = T)
return(fit = fit_svm)
}
system.time(tune150 <- tune.svm(x = train_cnn150[,-c(1,152)],
y = train_cnn150[,1],
data = train_cnn150,
cost = 10^c(-1:2)))
library(e1071)
system.time(tune150 <- tune.svm(x = train_cnn150[,-c(1,152)],
y = train_cnn150[,1],
data = train_cnn150,
cost = 10^c(-1:2)))
system.time(best.model <- train.fn(train_cnn150[,-c(1,152)], train_cnn150[,1], tune150$best))
tune150$best.parameters
system.time(best.model <- train.fn(train_cnn150[,-c(1,152)], train_cnn150[,1], tune150$best.parameters))
test.fn <- function(data_test, fit_model){
### Fit the classfication model with testing data
### Input:
###  - the fitted classification model using training data
###  -  processed features from testing images
### Output: training model specification
### load libraries
library(e1071)
pred <- predict(fit_model, data_test, probability = T)
pred.prob <- attr(pred, 'probabilities')
return(pred.prob)
}
system.time(pred150 <- test.fn(test_cnn150[,-c(1,152)],
best.model))
mean((pred150[,1] <= 0.5) == test_cnn150[,1])
system.time(tune150 <- tune.svm(x = train_cnn150[,-c(1,152)],
y = train_cnn150[,1],
data = train_cnn150,
cost = 10^c(-1:2),
kernel = 'linear'))
Jsmith <- data.frame(scan("Jsmith.txt",
what = list(coauthor ="", paper = "", publication =""),
sep = "<", quiet = T),stringsAsFactors = F)
Jsmith <- data.frame(scan("JSmith.txt",
what = list(coauthor ="", paper = "", publication =""),
sep = "<", quiet = T),stringsAsFactors = F)
setwd("/Users/ouminamikun/Documents/Columbia/Spring 2017/ADS/Spr2017-proj4-team-14/data/nameset")
Jsmith <- data.frame(scan("JSmith.txt",
what = list(coauthor ="", paper = "", publication =""),
sep = "<", quiet = T),stringsAsFactors = F)
setwd("/Users/ouminamikun/Documents/Columbia/Spring 2017/ADS/Spr2017-proj4-team-14/data/nameset")
Jsmith <- data.frame(scan("JSmith.txt",
what = list(coauthor ="", paper = "", publication =""),
sep = "<", quiet = T),stringsAsFactors = F)
View(Jsmith)
JSmith <- data.frame(scan("JSmith.txt",
what = list(coauthor ="", paper = "", publication =""),
sep = "<", quiet = T),stringsAsFactors = F)
JSmith$paper <- gsub(">", "", JSmith$paper)
JSmith$publication <- gsub(">","",JSmith$publication)
View(JSmith)
JSmith <- data.frame(scan("JSmith.txt",
what = list(coauthor ="", paper = "", publication =""),
sep = ">", quiet = T),stringsAsFactors = F)
JSmith$authors <- gsub("<","",sub("^.*?\\s","", JSmith$coauthor))
s <- strsplit(JSmith$authors, ";")
s
length(s)
for(i in 1:length(s)){
l <- length(s[[i]])
}
l
for(i in 1:length(s)){
l[i] <- length(s[[i]])
}
l
mean(l == 1)
unique(s)
library(e1071)
?naiveBayes
library(mlbench)
data("HouseVotes84")
View(HouseVotes84)
