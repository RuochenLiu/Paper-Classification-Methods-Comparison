---
title: "Naive Bayes Main Script"
author: "Nanjun Wang"
output:
      html_notebook:
      theme: journal
      toc: TRUE
---

## Step 0: Load the packages, specify directories
```{r}
setwd("~/Documents/Columbia/Spring 2017/ADS/Spr2017-proj4-team-14")
```

## Step 1: Load and process the data
In each record of the nameset, we need to extract the information of canonical author id and coauthors. We use cleaned data posted by the class and M matrix in paper 6 constructed by my teammate to accomplish that. 
```{r}
## Load data_list
load("../Spr2017-proj4-team-14/output/CleanData.RData")

## Load M matrix
load("../Spr2017-proj4-team-14/output/M.RData")

## Extract canonical author id
label <- function(data_list, num){
  nameset <- data_list[[num]]
  n <- length(nameset)
  label <- NA
  for(i in 1:n){
    label[i] <- nameset[[i]][1]
  }
  return(unlist(label))
}

labels <- NULL
for(i in 1:14){
  l <- label(data_list = data_list, num = i)
  labels[[i]] <- l
}

## Extract coauthors informartion and
## combine it with canonical author id
coauthors <- NULL
for(i in 1:14){
  obs <- length(data_list[[i]])
  d <- ncol(M.Matrix[[i]])
  labels[[i]] <- as.matrix(labels[[i]])
  coauthors[[i]] <- cbind(labels[[i]],M.Matrix[[i]][1:obs, (obs+1) : d])
  coauthors[[i]] <- cbind(coauthors[[i]], rep(1,obs))
}
```

## Step 2: Naive Bayes model
Following the section 2 in the paper, we use each author's past citations as the training data to estimate the model parameters. Based on the parameters estimates, we use the Bayes rule to calculate the probability that each name entry X$_i$ would have generated the input citation.

### 2.1 The model and its parameters
Given an input test citaiton C with the ommissuib of the query author, the traget function is to find a name entry X$_i$ in the citation database with the maximal posterior probability of producing the citation C, i.e., $max_iP(X_i|C) (1)$. After the decomposition of (1), it turns out in the training process we need to extimates seven parameters: $P(X_i)$, $P(N|X_i)$, $P(Co|X_i)$, $P(Seen|Co,X_i)$, $P(Unseen|Co,X_i)$, $P(A_1k|Seen, Co, X_i)$, $P(A_1k|Unseen, Co, X_i)$.
We have already split all records in the nameset for the experiments in section 4. Here, we only use A Gupta's training data as an example to show how training process goes.
```{r}
source("../Spr2017-proj4-team-14/lib/wang_nb_train.R")
train_1 <- read.csv("../Spr2017-proj4-team-14/output/Exp1/train_1.csv")
train_1 <- as.matrix(train_1)
model <- nb_train(train_x = train_1[,-1], train_y = train_1[,1])
model
```

### 2.2 Make prediction 
We still use A Gupta's test data as an example to show how test process goes
```{r}
source("../Spr2017-proj4-team-14/lib/wang_nb_test.R")
test_1 <- read.csv("../Spr2017-proj4-team-14/output/Exp1/train_1.csv")
prediction <- nb_test(model = model, test_data = test_1[,-1])
prediction
```

## Step 3: Evaluation experiments
Following the section 4, we split all records in the nameset into training data and test data as the way suggested in the paper. We conduct ten exmperiments on each record and calulate the mean and the standard deviation of accuracy.
```{r}
source("../Spr2017-proj4-team-14/lib/wang_read_files.R")
source("../Spr2017-proj4-team-14/lib/wang_experiments.R")

accuracy_table <- matrix(NA,2,14)
for(i in 1:14){
  nameset <- read_files(i)
  result <- experiment(nameset$train_files, nameset$test_files)
  accuracy_table[1,i] <- result$mean
  accuracy_table[2,i] <- result$StdDev
}
accuracy_table <- as.data.frame(accuracy_table)
colnames(accuracy_table) <- c("AGupta","Akumar", "CCchen","DJohnson", "JLee","JMartin","JRobinson","JSmith","KTanaka","MBrown","MJones","MMiller","SLee","Ychen")
rownames(accuracy_table) <- c("Mean", "StdDev")
accuracy_table
```

## Step 4: In-paper comparison
```{r}

```