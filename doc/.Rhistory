}
head(label2, 20)
head(label,20)
head(label,100:120)
label[100:120]
label2[100:120]
Train <- function(data.train, M, k, t){
#### Initialization
n <- nrow(data.train) #### Number of publications
m.matrix <- M
for(i in 1:t){
m.matrix <- m.matrix%*%M
}
m.matrix <- m.matrix[1:n, 1:n]
w <- 0.7^t
label2 <- vector("numeric", n) #### Store train_labels
label1 <- vector("numeric", n) #### For comparison
y <- matrix(0, nrow = k, ncol = ncol(data.train))
A <- matrix(0, nrow = ncol(data.train), ncol = ncol(data.train))
diag(A) <- rep(1,nrow(A))
#### D function
D <- function(xi,xj,A){
s <- 1 - as.numeric( (t(xi) %*% A %*% xj)/ sqrt(t(xi) %*% A %*% xi)/ sqrt(t(xj) %*% A %*% xj) )
return(s)
}
#### Initial assignments (l1)
label2 <- sample(1:k, n, replace =  TRUE)
#### Initialize y values
for(i in 1:k){
y[i,] <- apply(data.train[(label2 == i),], 2, sum)/sum(label2 == i)
}
#### Initialize A matirx
diag(A) <- rep(1,nrow(A))
#### Iteration functions
while(sum(label1 != label2) > n/50){
label1 <- label2
#### E step
for(i in 1:n){
iter.value <- vector("numeric", k)
for(j in 1:k){
label2[i] <- j
fobj <- 0
for(l in 1:n){
fobj <- fobj + D(data.train[i,],data.train[l,],A)*(label2[i]!=label2[l])*w*(m.matrix[i,l] >0)
}
iter.value[j] <- fobj + D(data.train[i,],y[j,],A)
}
label2[i] <- which.min(iter.value)
}
#### M step
#### Update y
for(i in 1:k){
y[i,] <- apply(data.train[(label2 == i),], 2, sum)/sum(label2 == i)
}
#### Update A matrix
for(i in 1:nrow(A)){
}
}
return(label2)
}
Train(data.train, M, k, t)
require("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
source("../lib/M.R")
load("../output/CleanData.RData")
load("../output/M.RData")
source("../lib/C6_train.R")
source('../lib/evaluation_measures.R')
d <- data_list[[10]]
M <- M.Matrix[[10]]
n <- length(d) #### Number of publications
paper <- vector("character", length = n)
for(i in 1:n){
paper[i] <- d[[i]][[5]]
}
it_train <- itoken(paper,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
"at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
train.data <- as.matrix(fit_transform(dtm_train, tfidf)) #### Get train data features.
data.train <- train.data
t <- 3
label <- vector("numeric", length = n) #### Save true assignments.
for(i in 1:n){
label[i] <- d[[i]][[1]]
}
k <- length(unique(label)) #### Number of clusters.
train_label <- Train(train.data, M, k, t)
train_label <- Train(train.data, M, k, t)
require("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
source("../lib/M.R")
load("../output/CleanData.RData")
load("../output/M.RData")
source("../lib/C6_train.R")
source('../lib/evaluation_measures.R')
#### Get Features
d <- data_list[[10]]
M <- M.Matrix[[10]]
n <- length(d) #### Number of publications
paper <- vector("character", length = n)
for(i in 1:n){
paper[i] <- d[[i]][[5]]
}
it_train <- itoken(paper,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
"at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
train.data <- as.matrix(fit_transform(dtm_train, tfidf)) #### Get train data features.
data.train <- train.data
t <- 3
label <- vector("numeric", length = n) #### Save true assignments.
for(i in 1:n){
label[i] <- d[[i]][[1]]
}
k <- length(unique(label)) #### Number of clusters.
#### Train
train_label <- Train(train.data, M, k, t)
#### Result
matching_matrix_c6 <- matching_matrix(label,train_label)
performance_c6 <- performance_statistics(matching_matrix_c6)
performance_c6
data.train <- train.data
t <- 3
label <- vector("numeric", length = n) #### Save true assignments.
for(i in 1:n){
label[i] <- d[[i]][[1]]
}
k <- length(unique(label)) #### Number of clusters.
train_label <- Train(train.data, M, k, t)
require("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
source("../lib/M.R")
load("../output/CleanData.RData")
load("../output/M.RData")
source("../lib/C6_train.R")
source('../lib/evaluation_measures.R')
#### Get Features
d <- data_list[[10]]
M <- M.Matrix[[10]]
n <- length(d) #### Number of publications
paper <- vector("character", length = n)
for(i in 1:n){
paper[i] <- d[[i]][[5]]
}
it_train <- itoken(paper,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
"at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
train.data <- as.matrix(fit_transform(dtm_train, tfidf)) #### Get train data features.
data.train <- train.data
t <- 3
label <- vector("numeric", length = n) #### Save true assignments.
for(i in 1:n){
label[i] <- d[[i]][[1]]
}
k <- length(unique(label)) #### Number of clusters.
#### Train
train_label <- Train(train.data, M, k, t)
#### Result
matching_matrix_c6 <- matching_matrix(label,train_label)
performance_c6 <- performance_statistics(matching_matrix_c6)
n <- nrow(data.train) #### Number of publications
f <- ncol(data.train) #### Number of features
m.matrix <- M
for(i in 1:t){
m.matrix <- m.matrix%*%M
}
m.matrix <- m.matrix[1:n, 1:n]
w <- 0.7^t
label2 <- vector("numeric", n) #### Store train_labels
label1 <- vector("numeric", n) #### For comparison
y <- matrix(0, nrow = k, ncol = f)
A <- matrix(0, nrow = f, ncol = f)
diag(A) <- rep(1,nrow(A))
D <- function(xi,xj,A){
s <- 1 - as.numeric( (t(xi) %*% A %*% xj)/ sqrt(t(xi) %*% A %*% xi)/ sqrt(t(xj) %*% A %*% xj) )
return(s)
}
Anorm <- function(xi,A){
return(sqrt(t(xi) %*% A %*% xi))
}
gradient <- function(xi,xj,A){
g <- vector("numeric", nrow(A))
for(i in 1:nrow(A)){
part1 <- xi[i]*xj[i]*Anorm(xi,A)*Anorm(xj,A)
part2 <- t(xi)%*%A%*%xj*((xi[i]^2)*(Anorm(xi,A)^2) + (xj[i]^2)*(Anorm(xj,A)^2))/2/Anorm(xi,A)/Anorm(xj,A)
part3 <- (Anorm(xi,A)^2)*(Anorm(xj,A)^2)
g[i] <- (part1 - part2)/part3
}
return(g)
}
while(length(table(label2)) > n/50){
label2 <- sample(1:k, n, replace =  TRUE)
}
for(i in 1:k){
if(sum(label2 == i) > 1){
y[i,] <- apply(data.train[(label2 == i),], 2, sum)/sum(label2 == i)
}
else if(sum(label2 == i) == 1){
y[i,] <- data.train[(label2 == i),]
}
else{
y[i,] <- rep(0,f)
}
}
diag(A) <- rep(1,nrow(A))
while(sum(label1 != label2) > 0){
label1 <- label2
#### E step
for(i in 1:n){
iter.value <- rep(0,k)
for(j in 1:k){
label2[i] <- j
fobj <- 0
for(l in 1:n){
fobj <- fobj + D(data.train[i,],data.train[l,],A)*(label2[i]!=label2[l])*w*(m.matrix[i,l] >0)
}
iter.value[j] <- fobj + D(data.train[i,],y[j,],A)
}
label2[i] <- which.min(iter.value)
}
#### M step
#### Update y
for(i in 1:k){
if(sum(label2 == i) > 1){
y[i,] <- apply(data.train[(label2 == i),], 2, sum)/sum(label2 == i)
}
if(sum(label2 == i) == 1){
y[i,] <- data.train[(label2 == i),]
}
if(sum(label2 == i) == 0){
y[i,] <- rep(0,f)
}
}
#### Update A matrix
delta <- rep(0,f)
part1 <- matrix(nrow = n, ncol = f)
for(i in 1:n){
if(sum(label2 == label2[i]) > 1){
xa <- apply(data.train[label2 == label2[i], ], 2, sum)
}
if(sum(label2 == label2[i]) == 1){
xa <- data.train[label2 == label2[i], ]
}
if(sum(label2 == label2[i]) == 0){
xa <- rep(0,f)
}
part1[i,] <- gradient(data.train[i,], y[label2[i],]*sqrt(t(xa) %*% A %*% xa), A)
}
part1 <- colSums(part1)
part2 <- rep(0,f)
for(i in 1:n){
for(j in 1:n){
if(m.matrix[i,j]>0 & label2[i] != label2[j]){
part2 <- part2 + gradient(data.train[i,], data.train[j,], A)
}
}
}
delta <- part1 + part2
for(i in 1:f){
A[i,i] <- A[i,i] + 0.001*delta[i]
}
}
while(length(table(label2)) > n/50){
label2 <- sample(1:k, n, replace =  TRUE)
}
while(length(table(label2)) < k){
label2 <- sample(1:k, n, replace =  TRUE)
}
table(label2)
for(i in 1:k){
if(sum(label2 == i) > 1){
y[i,] <- apply(data.train[(label2 == i),], 2, sum)/sum(label2 == i)
}
else if(sum(label2 == i) == 1){
y[i,] <- data.train[(label2 == i),]
}
else{
y[i,] <- rep(0,f)
}
}
diag(A) <- rep(1,nrow(A))
while(sum(label1 != label2) > n/50){
label1 <- label2
#### E step
for(i in 1:n){
iter.value <- rep(0,k)
for(j in 1:k){
label2[i] <- j
fobj <- 0
for(l in 1:n){
fobj <- fobj + D(data.train[i,],data.train[l,],A)*(label2[i]!=label2[l])*w*(m.matrix[i,l] >0)
}
iter.value[j] <- fobj + D(data.train[i,],y[j,],A)
}
label2[i] <- which.min(iter.value)
}
#### M step
#### Update y
for(i in 1:k){
if(sum(label2 == i) > 1){
y[i,] <- apply(data.train[(label2 == i),], 2, sum)/sum(label2 == i)
}
if(sum(label2 == i) == 1){
y[i,] <- data.train[(label2 == i),]
}
if(sum(label2 == i) == 0){
y[i,] <- rep(0,f)
}
}
#### Update A matrix
delta <- rep(0,f)
part1 <- matrix(nrow = n, ncol = f)
for(i in 1:n){
if(sum(label2 == label2[i]) > 1){
xa <- apply(data.train[label2 == label2[i], ], 2, sum)
}
if(sum(label2 == label2[i]) == 1){
xa <- data.train[label2 == label2[i], ]
}
if(sum(label2 == label2[i]) == 0){
xa <- rep(0,f)
}
part1[i,] <- gradient(data.train[i,], y[label2[i],]*sqrt(t(xa) %*% A %*% xa), A)
}
part1 <- colSums(part1)
part2 <- rep(0,f)
for(i in 1:n){
for(j in 1:n){
if(m.matrix[i,j]>0 & label2[i] != label2[j]){
part2 <- part2 + gradient(data.train[i,], data.train[j,], A)
}
}
}
delta <- part1 + part2
for(i in 1:f){
A[i,i] <- A[i,i] + 0.001*delta[i]
}
}
require("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
source("../lib/M.R")
load("../output/CleanData.RData")
load("../output/M.RData")
source("../lib/C6_train.R")
source('../lib/evaluation_measures.R')
#### Get Features
d <- data_list[[10]]
M <- M.Matrix[[10]]
n <- length(d) #### Number of publications
paper <- vector("character", length = n)
for(i in 1:n){
paper[i] <- d[[i]][[5]]
}
it_train <- itoken(paper,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
"at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
train.data <- as.matrix(fit_transform(dtm_train, tfidf)) #### Get train data features.
data.train <- train.data
t <- 3
label <- vector("numeric", length = n) #### Save true assignments.
for(i in 1:n){
label[i] <- d[[i]][[1]]
}
k <- length(unique(label)) #### Number of clusters.
#### Train
train_label <- Train(train.data, M, k, t)
#### Result
matching_matrix_c6 <- matching_matrix(label,train_label)
performance_c6 <- performance_statistics(matching_matrix_c6)
performance_c6
save(performance_c6, file = "../output/result_paper10_c6.RData")
require("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
source("../lib/M.R")
load("../output/CleanData.RData")
load("../output/M.RData")
source("../lib/C6_train.R")
source('../lib/evaluation_measures.R')
#### Get Features
d <- data_list[[1]]
M <- M.Matrix[[1]]
n <- length(d) #### Number of publications
paper <- vector("character", length = n)
for(i in 1:n){
paper[i] <- d[[i]][[5]]
}
it_train <- itoken(paper,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
"at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
train.data <- as.matrix(fit_transform(dtm_train, tfidf)) #### Get train data features.
data.train <- train.data
t <- 3
label <- vector("numeric", length = n) #### Save true assignments.
for(i in 1:n){
label[i] <- d[[i]][[1]]
}
k <- length(unique(label)) #### Number of clusters.
#### Train
train_label <- Train(train.data, M, k, t)
#### Result
matching_matrix_c6 <- matching_matrix(label,train_label)
performance_c6 <- performance_statistics(matching_matrix_c6)
save(performance_c6, file = "../output/result_paper10_c6.RData")
require("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
source("../lib/M.R")
load("../output/CleanData.RData")
load("../output/M.RData")
source("../lib/C6_train.R")
source('../lib/evaluation_measures.R')
#### Get Features
d <- data_list[[10]]
M <- M.Matrix[[10]]
n <- length(d) #### Number of publications
paper <- vector("character", length = n)
for(i in 1:n){
paper[i] <- d[[i]][[5]]
}
it_train <- itoken(paper,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
"at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
train.data <- as.matrix(fit_transform(dtm_train, tfidf)) #### Get train data features.
data.train <- train.data
t <- 5
label <- vector("numeric", length = n) #### Save true assignments.
for(i in 1:n){
label[i] <- d[[i]][[1]]
}
k <- length(unique(label)) #### Number of clusters.
#### Train
train_label <- Train(train.data, M, k, t)
require("pacman")
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
source("../lib/M.R")
load("../output/CleanData.RData")
load("../output/M.RData")
source("../lib/C6_train.R")
source('../lib/evaluation_measures.R')
#### Get Features
d <- data_list[[10]]
M <- M.Matrix[[10]]
n <- length(d) #### Number of publications
paper <- vector("character", length = n)
for(i in 1:n){
paper[i] <- d[[i]][[5]]
}
it_train <- itoken(paper,
preprocessor = tolower,
tokenizer = word_tokenizer,
progressbar = FALSE)
vocab <- create_vocabulary(it_train, stopwords = c("a", "an", "the", "in", "on",
"at", "of", "above", "under"))
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
train.data <- as.matrix(fit_transform(dtm_train, tfidf)) #### Get train data features.
data.train <- train.data
t <- 5
label <- vector("numeric", length = n) #### Save true assignments.
for(i in 1:n){
label[i] <- d[[i]][[1]]
}
k <- length(unique(label)) #### Number of clusters.
#### Train
train_label <- Train(train.data, M, k, t)
#### Result
matching_matrix_c6 <- matching_matrix(label,train_label)
performance_c6 <- performance_statistics(matching_matrix_c6)
save(performance_c6, file = "../output/result_paper10_c6.RData")
performance_c6
