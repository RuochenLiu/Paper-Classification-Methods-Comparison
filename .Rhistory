# train result
ak.train <- p.journal(train, df, j_p = j_p)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
return(result.class)
}
acc.test <- function(df, j_p) {
result.class <- cluster.test(df, j_p = j_p)
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
final.j <- acc.test(df, "j")
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/output")
attach("CleanData.RData")
index <- read.csv("random_agupta.csv")
index
index <- index$x
train <- df[index, ]
test <- df[-index, ]
ak.train <- p.journal(train, df, j_p = j_p)
ak.train <- p.journal(train, df, j_p = "j")
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
result.class
acc.j <- mean(as.numeric(result.class) == test$clusterid)
acc.j
write.csv(acc.j, file = "~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/output/agupta.j.pred.csv")
train.p <- p.journal(train, df, j_p = "p")
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train.p[names(subdic), ] * train.p["p.title.seen", ] +
train.p["p.word.unseen", ] * train.p["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train.p["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
acc.j <- mean(as.numeric(result.class) == test$clusterid)
write.csv(acc.j, file = "~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/output/agupta.p.pred.csv")
ak.train <- p.journal(train, df, j_p = "j")
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
acc.j <- mean(as.numeric(result.class) == test$clusterid)
write.csv(result.class, file = "~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/output/agupta.j.pred.csv")
train.p <- p.journal(train, df, j_p = "p")
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train.p[names(subdic), ] * train.p["p.title.seen", ] +
train.p["p.word.unseen", ] * train.p["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train.p["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
acc.j <- mean(as.numeric(result.class) == test$clusterid)
write.csv(result.class, file = "~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/output/agupta.p.pred.csv")
q1 <- result.class
train.j <- p.journal(train, df, j_p = "j")
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train.j[names(subdic), ] * train.j["p.title.seen", ] +
train.j["p.word.unseen", ] * train.j["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train.j["prior.author", ]
}
result.class.j <- author[apply(post.p, 1, which.max)]
acc.j <- mean(as.numeric(result.class) == test$clusterid)
train.p <- p.journal(train, df, j_p = "p")
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train.p[names(subdic), ] * train.p["p.title.seen", ] +
train.p["p.word.unseen", ] * train.p["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train.p["prior.author", ]
}
result.class.p <- author[apply(post.p, 1, which.max)]
acc.p <- mean(as.numeric(result.class) == test$clusterid)
q1==result.class.p
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/libevaluation_measures.R')
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/libevaluation_measures.R', local = T)
matching_matrix <- function(G,M){
### Compute the matching matrix following the paper Kang et at.(2009)
### The notations are same as in the paper
### Input: G: Gold standard clusters, numeric vector, same length as number of citations
###        M: Machine generated clusters, numeric vector, same length as G
### Output: matching matrix as decribed in paper Kang et at.(2009)
### n is the number of citations
n <- length(G)
### initialize the matching matrix
result_matrix <- matrix(rep(0,4),ncol=2,nrow=2)
### compute pariwise agreement between two partitions
for(i in 1:(n-1)){
for(j in (i+1):n){
if(G[i]==G[j]&M[i]==M[j]) result_matrix[1,1]<-result_matrix[1,1]+1
if(G[i]!=G[j]&M[i]==M[j]) result_matrix[1,2]<-result_matrix[1,2]+1
if(G[i]==G[j]&M[i]!=M[j]) result_matrix[2,1]<-result_matrix[2,1]+1
if(G[i]!=G[j]&M[i]!=M[j]) result_matrix[2,2]<-result_matrix[2,2]+1
}
}
return(result_matrix)
}
performance_statistics <- function(result_matrix){
### Compute the four evaluation measures following the paper Kang et at.(2009)
### The notations are same as in the paper
### Input: result_matrix: matching matrix, which is an output from above function
### Output: four evaluation statistics
### Precision = TP/(TP+FP)
precision <- result_matrix[1,1]/(result_matrix[1,1]+result_matrix[1,2])
### Recall = TP/(TP+FN)
recall <- result_matrix[1,1]/(result_matrix[1,1]+result_matrix[2,1])
### F1 is 2*harmonic mean of presicion and recall
f1 <- 2*precision*recall/(precision+recall)
### Accuracy is the fraction of agreements
accuracy <- (result_matrix[1,1]+result_matrix[2,2])/sum(result_matrix)
return(list(precision=precision, recall=recall, f1=f1, accuracy=accuracy))
}
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/libevaluation_measures.R', local = T)
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/lib/evaluation_measures.R', local = T)
matching_matrix_hclust <- matching_matrix(df$clusterid,result.class.j)
matching_matrix_hclust <- matching_matrix(df$clusterid[index],result.class.j)
matching_matrix_hclust <- matching_matrix(result.class.p,result.class.j)
performance_hclust <- performance_statistics(matching_matrix_hclust)
performance_hclust
matching_matrix_sclust <- matching_matrix(as.numeric(df$clusterid[index]), result.class.p)
as.numeric(df$clusterid[index])
result.class.p
matching_matrix_sclust <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.p)
matching_matrix_hclust <- matching_matrix(as.numeric(df$clusterid[-index]),result.class.j)
performance_hclust <- performance_statistics(matching_matrix_hclust)
matching_matrix_sclust <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.p)
performance_sclust <- performance_statistics(matching_matrix_sclust)
compare_df <- data.frame(method = c("sClust", "hClust"),
precision = c(performance_sclust$precision, performance_hclust$precicion),
recall = c(performance_sclust$recall, performance_hclust$recall),
f1 = c(performance_sclust$f1, performance_hclust$f1),
accuracy=c(performance_sclust$accuracy, performance_hclust$accurauracy),
time=c(time_sclust,time_hclust))
compare_df <- data.frame(method = c("sClust", "hClust"),
precision = c(performance_sclust$precision, performance_hclust$precicion),
recall = c(performance_sclust$recall, performance_hclust$recall),
f1 = c(performance_sclust$f1, performance_hclust$f1),
accuracy=c(performance_sclust$accuracy, performance_hclust$accurauracy))
kable(compare_df,caption = "Comparision of performance for two clustering methods", digits = 2)
compare_df
matching_matrix_journal <- matching_matrix(as.numeric(df$clusterid[-index]),result.class.j)
performance_journal <- performance_statistics(matching_matrix_journal)
matching_matrix_paper <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.p)
performance_paper <- performance_statistics(matching_matrix_paper)
compare_df <- data.frame(method = c("paper", "journal"),
precision = c(performance_paper$precision, performance_journal$precicion),
recall = c(performance_paper$recall, performance_journal$recall),
f1 = c(performance_paper$f1, performance_journal$f1),
accuracy=c(performance_paper$accuracy, performance_journal$accurauracy))
,
time=c(time_paper,time_journal))
kable(compare_df,caption = "Comparision of performance for two clustering methods", digits = 2)
compare_df
result.class.c <- read.csv("coauthor_pred.csv")
result.class.c <- result.class.c$x
matching_matrix_coauthor <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.c)
performance_coauthor <- performance_statistics(matching_matrix_coauthor)
matching_matrix_journal <- matching_matrix(as.numeric(df$clusterid[-index]),result.class.j)
performance_journal <- performance_statistics(matching_matrix_journal)
matching_matrix_paper <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.p)
performance_paper <- performance_statistics(matching_matrix_paper)
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precicion),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy=c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accurauracy))
compare_df
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precicion),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy=c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accurauracy))
result.class.c <- read.csv("coauthor_pred.csv")
result.class.c <- result.class.c$x
matching_matrix_coauthor <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.c)
performance_coauthor <- performance_statistics(matching_matrix_coauthor)
matching_matrix_journal <- matching_matrix(as.numeric(df$clusterid[-index]),result.class.j)
performance_journal <- performance_statistics(matching_matrix_journal)
matching_matrix_paper <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.p)
performance_paper <- performance_statistics(matching_matrix_paper)
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precicion),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accurauracy))
c(performance_coauthor$precision, performance_paper$precision, performance_journal$precicion)
performance_coauthor
performance_coauthor$precision
performance_paper$precision
performance_journal$precicion
matching_matrix_journal <- matching_matrix(as.numeric(df$clusterid[-index]),result.class.j)
performance_journal <- performance_statistics(matching_matrix_journal)
performance_journal
performance_journal$precicion
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accurauracy))
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
#precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accurauracy))
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accurauracy))
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision)
precision
method = c("coauthor", "paper", "journal")
method
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall)
recall
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1)
f1
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accurauracy)
accuracy
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accuracy)
accuracy
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accuracy))
compare_df
time.j <- function()
{# train result for journal title
train.j <- p.journal(train, df, j_p = "j")
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train.j[names(subdic), ] * train.j["p.title.seen", ] +
train.j["p.word.unseen", ] * train.j["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train.j["prior.author", ]
}
result.class.j <- author[apply(post.p, 1, which.max)]
acc.j <- mean(as.numeric(result.class) == test$clusterid)}
time.j()
time.jour <- time.j()
time.jour
time.p <- function()
{train.p <- p.journal(train, df, j_p = "p")
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train.p[names(subdic), ] * train.p["p.title.seen", ] +
train.p["p.word.unseen", ] * train.p["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train.p["prior.author", ]
}
result.class.p <- author[apply(post.p, 1, which.max)]
acc.p <- mean(as.numeric(result.class) == test$clusterid)}
time.paper <- time.p()
time.paper
time.jour
time.co <- 3.100
time.coauthor <- 3.100
time.journal <- time.j()
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accuracy)),
time = c(time.coauthor, time.paper, time.journal))
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accuracy),
time = c(time.coauthor, time.paper, time.journal))
compare_df
?Sys.time
Sys.time()
Sys.time()
system.time()
acc.test <- function(df, j_p) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
ak.train <- p.journal(train, df, j_p = j_p)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(ak.train[names(subdic), ] * ak.train["p.title.seen", ] +
ak.train["p.word.unseen", ] * ak.train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * ak.train["prior.author", ]
}
output <- author[apply(post.p, 1, which.max)]
#  output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
train <- p.journal(train, df, j_p = j_p)
train <- df[index, ]
train <- p.journal(train, df, "j")
dim(train)
test.result <- function(index, df, j_p) {
# train result
train <- df[index, ]
test <- df[-index, ]
train <- p.journal(train, df, j_p = j_p)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train[names(subdic), ] * train["p.title.seen", ] +
train["p.word.unseen", ] * train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train["prior.author", ]
}
output <- author[apply(post.p, 1, which.max)]
return(output)
}
test.result(index, df, "j")
time.journal <- system.time(test.result(index, df, "j"))
time.journal
time.p <- function()
{train.p <- p.journal(train, df, j_p = "p")
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train.p[names(subdic), ] * train.p["p.title.seen", ] +
train.p["p.word.unseen", ] * train.p["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train.p["prior.author", ]
}
result.class.p <- author[apply(post.p, 1, which.max)]
acc.p <- mean(as.numeric(result.class) == test$clusterid)}
system.time(time.p())
system.time(time.p
)
result.class.p <- test.result(index, df, "p")
time.paper <- system.time(test.result(index, df, "p"))
time.paper
time.paper[3]
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accuracy),
time = c(time.coauthor, time.paper[3], time.journal[3]))
compare_df
?kable
install.packages("knitr")
library(knitr)
?kable
kable(compare_df,caption = "Comparision of performance for two clustering methods", digits = 2)
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/lib/journal.result.R', local = T)
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/lib/journal.result.R', local = T)
df <- trans.data(j)
for(i in 1:10) {
final.j[i, j] <- acc.test(df, "j")
}
j=1
df <- trans.data(j)
final.j <- matrix(NA, nrow = 10, ncol = 14)
for(i in 1:10) {
final.j[i, j] <- acc.test(df, "j")
}
i=1
acc.test(df, "j")
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
train <- p.journal(train, df, j_p = j_p)
j_p = "j"
train <- p.journal(train, df, j_p = j_p)
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train[names(subdic), ] * train["p.title.seen", ] +
train["p.word.unseen", ] * train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
acc.test(df, "j")
acc.test <- function(df, j_p) {
# split data
index <- split_data(df)
train <- df[index, ]
test <- df[-index, ]
# train result
train <- p.journal(train, df, j_p = j_p)
# calculate posterior probability for each author
post.p <- matrix(NA, nrow = nrow(test), ncol = K)
for (k in 1:nrow(test)) {
subdic <- word.infor(test[k, ])
# number of different words in sub-dictionary
n.word <- length(subdic)
p <- matrix(train[names(subdic), ] * train["p.title.seen", ] +
train["p.word.unseen", ] * train["p.title.unseen", ], ncol = K)
post.p[k,] <- apply(p, 2, prod) * train["prior.author", ]
}
result.class <- author[apply(post.p, 1, which.max)]
output <- mean(as.numeric(result.class) == test$clusterid)
return(output)
}
acc.test(df,"j")
acc.test(df, "j")
final.j[i, j] <- acc.test(df, "j")
for(i in 1:10) {
final.j[i, j] <- acc.test(df, "j")
}
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/lib/comparison result.R', local = T)
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/lib/evaluation_measures.R', local = T)
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/lib/comparison result.R', local = T)
result.class.c <- read.csv("coauthor_pred.csv")
result.class.c <- result.class.c$x
matching_matrix_coauthor <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.c)
performance_coauthor <- performance_statistics(matching_matrix_coauthor)
matching_matrix_journal <- matching_matrix(as.numeric(df$clusterid[-index]),result.class.j)
performance_journal <- performance_statistics(matching_matrix_journal)
matching_matrix_paper <- matching_matrix(as.numeric(df$clusterid[-index]), result.class.p)
performance_paper <- performance_statistics(matching_matrix_paper)
compare_df <- data.frame(method = c("coauthor", "paper", "journal"),
precision = c(performance_coauthor$precision, performance_paper$precision, performance_journal$precision),
recall = c(performance_coauthor$recall, performance_paper$recall, performance_journal$recall),
f1 = c(performance_coauthor$f1, performance_paper$f1, performance_journal$f1),
accuracy = c(performance_coauthor$accuracy, performance_paper$accuracy, performance_journal$accuracy),
time = c(time.coauthor, time.paper[3], time.journal[3]))
compare_df
kable(compare_df,caption = "Comparision of performance for two clustering methods", digits = 2)
if (!require(c("NLP", "tm"))) install.packages(c("NLP", "tm"))
install.packages(c("NLP", "tm"))
if (!require(c("NLP", "tm"))) install.packages(c("NLP", "tm"))
install.packages(c("NLP", "tm"))
install.packages(c("NLP", "tm")
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14")
install.packages(c("NLP", "tm"))
require(c("NLP", "tm"))
library(c("NLP", "tm"))
install.packages(c("NLP", "tm"))
library("NLP")
library("tm")
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14")
# here replace it with your own path or manually set it in RStudio
# to where this rmd file is located
# install.packages(c("NLP", "tm"))
library("NLP")
library("tm")
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14")
# here replace it with your own path or manually set it in RStudio
# to where this rmd file is located
# install.packages(c("NLP", "tm"))
library("NLP")
library("tm")
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14")
# here replace it with your own path or manually set it in RStudio
# to where this rmd file is located
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14")
# install.packages(c("NLP", "tm"))
library("NLP")
library("tm")
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14")
# here replace it with your own path or manually set it in RStudio
# to where this rmd file is located
source('~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/lib/p.function.R', local = T)
AGupta <- trans.data(1)
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj4-team-14/output")
attach("CleanData.RData")
